{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "import copy\n",
    "import datetime as dt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('/Users/Hooshmand/Desktop/Kaggle_Animal_shelter/train.csv')\n",
    "#rename data_train 'AnimalID' column into 'ID' to match the test set\n",
    "data_train = data_train.rename(columns={'AnimalID':'ID'})\n",
    "data_test = pd.read_csv('/Users/Hooshmand/Desktop/Kaggle_Animal_shelter/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def animal_feature_transform(dataframe, has_outcome=True):\n",
    "    \n",
    "    data_train = copy.deepcopy(dataframe)\n",
    "    \n",
    "    # AnimalType: 1 - Cat, 0 - Dog\n",
    "    NA = np.nan\n",
    "    data_train['AnimalType'] = data_train['AnimalType'].map(lambda x: 1 if x == 'Cat' else (0 if x == 'Dog' else NA))\n",
    "    \n",
    "    #split SexuponOutcome into 2 columns: Sex (Male/Female), and S/N (Spayed/Neutered), then delete SexuponOutcome column\n",
    "    data_train['Sex'] = data_train['SexuponOutcome'].str.split().str[-1]\n",
    "    data_train['S/N'] = data_train['SexuponOutcome'].str.split().str[0]\n",
    "    data_train = data_train.drop('SexuponOutcome', 1)\n",
    "    \n",
    "    #Sex: 1 - Male, 0 - Female, NA - if missing\n",
    "    data_train['Sex'] = data_train['Sex'].map(lambda x: 1 if x == 'Male' else (0 if x == 'Female' else NA))\n",
    "    \n",
    "    # S/N: 1 - if spayed/neutered, 0 - if not, NA if missing\n",
    "    data_train['S/N'] = data_train['S/N'].map(lambda x: 1 if ((x == 'Neutered') | (x == 'Spayed')) else (0 if x == 'Intact' else NA))\n",
    "    \n",
    "    # split DateTime into 3 columns: Year, Month, Day\n",
    "    data_train['DateTime'] = data_train['DateTime'].str.split().str[0]\n",
    "    data_train['Year'] = data_train['DateTime'].str.split('-').str[0]\n",
    "    data_train['Month'] = data_train['DateTime'].str.split('-').str[1]\n",
    "    #data_train['Day'] = data_train['DateTime'].str.split('-').str[2]\n",
    "    data_train['Hour'] = data_train['DateTime'].map(lambda x: 12 if len(x) == 10 else dt.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    data_train['Day'] = data_train['DateTime'].map(lambda x: dt.datetime.strptime(x, \"%Y-%m-%d\").weekday() if len(x) == 10 else dt.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").weekday())\n",
    "    data_train = data_train.drop('DateTime', 1)\n",
    "    \n",
    "    # create column of ages: weeks, months, years\n",
    "    data_train['AgeUnit'] = data_train['AgeuponOutcome'].str.split().str[-1]\n",
    "    data_train['AgeUnit'] = data_train['AgeUnit'].map(lambda x: 52 if (x == 'year' or x == 'years') else (4 if (x == 'month' or x == 'months') else 1))\n",
    "    # create a column of numerical age\n",
    "    data_train['AgeCoeff'] = data_train['AgeuponOutcome'].str[0]\n",
    "    # convert the column above into numeric\n",
    "    data_train['AgeCoeff'] = data_train['AgeCoeff'].convert_objects(convert_numeric=True)\n",
    "    # create a column of tuples\n",
    "    data_train['Age']= data_train['AgeCoeff'] * data_train['AgeUnit']\n",
    "    \n",
    "    #Color\n",
    "    ##########################################################################################################\n",
    "    data_train['Black/White'] = data_train['Color'].map(lambda x:1 if ('Black/White' in x or 'White/Black' in x) else 0)\n",
    "    data_train['Tricolor'] = data_train['Color'].map(lambda x:1 if ('Calico' in x or 'Tricolor' in x or 'Tortie/White' in x or 'Torbie/White' in x) else 0)\n",
    "    data_train['BW Tbb/BW Tbb White'] = data_train['Color'].map(lambda x:1 if ('Brown Tabby' in x or 'Brown Tabby/White' in x or 'White/Brown Tabby' in x) else 0)\n",
    "\n",
    "    data_train['White'] = data_train['Color'].map(lambda x:1 if (x=='White') else 0)\n",
    "    data_train['Black'] = data_train['Color'].map(lambda x:1 if (x=='Black') else 0)\n",
    "    data_train['Org Tbb/Org Tbb White'] = data_train['Color'].map(lambda x:1 if (x=='Orange Tabby' or x=='Orange Tabby/White' or x=='Cream Tabby') else 0)\n",
    "    \n",
    "    data_train['Brown'] = data_train['Color'].map(lambda x:1 if (x=='Brown' or x=='Brown Brindle' or x=='Chocolate') else 0)\n",
    "    data_train['Tortoise/Blk BW'] = data_train['Color'].map(lambda x:1 if (x=='Black/Tan' or x=='Red/Black' or x == 'Tortie' or x == 'Torbie' or x=='Black/Brown' or x=='Brown/Black' or x=='Sable' or x=='Brown Brindle/White') else 0)\n",
    "    data_train['Red/Red White'] = data_train['Color'].map(lambda x:1 if ('Red' in x or 'Red/White' in x) else 0)\n",
    "    \n",
    "    data_train['Tan'] = data_train['Color'].map(lambda x:1 if (x=='Tan' or x=='Gold' or x=='Cream' or x=='Buff' or x=='Buff/Tan' or x=='Yellow' or x=='Tan/Cream' or x=='Tan/Tan') else 0)\n",
    "    data_train['BW White/Seal Pt'] = data_train['Color'].map(lambda x:1 if ('White/Brown' in x or 'Brown/White' in x or 'Seal Point' in x or 'Lynx Point' in x or 'Chocolate/White' in x or 'White/Chocolate' in x or 'White/Brown Brindle' in x) else 0)\n",
    "    data_train['Blue'] = data_train['Color'].map(lambda x:1 if (x=='Blue' or x=='Grey' or x=='Blue Merle') else 0)\n",
    "    \n",
    "    data_train['Blue Tbb/Blue Tbb White'] = data_train['Color'].map(lambda x:1 if (x=='Blue Tabby' or x=='Blue Tabby/White' or x=='Blue/White' or x=='White/Blue' or x=='White/Gray') else 0)\n",
    "    data_train['BW/White'] = data_train['Color'].map(lambda x:1 if (x=='Brown/White' or x=='White/Brown' or x=='White/Brown Brindle') else 0)\n",
    "    data_train['White/Tan'] = data_train['Color'].map(lambda x:1 if (x=='White/Tan' or x=='Tan/White') else 0)\n",
    "\n",
    "    #Breed\n",
    "    ##########################################################################################################\n",
    "    data_train['Domes Sh/h'] = data_train['Breed'].map(lambda x:1 if (x == 'Domestic Shorthair Mix' or 'Manx' in x) else 0)\n",
    "    data_train['Domes M/h'] = data_train['Breed'].map(lambda x:1 if 'Domestic Medium' in x else 0)\n",
    "    data_train['Domes L/h'] = data_train['Breed'].map(lambda x:1 if 'Domestic Longhair' in x else 0)\n",
    "    data_train['Siamese/Snowshow'] = data_train['Breed'].map(lambda x:1 if ('Siamese' in x or 'Snowshow' in x) else 0)\n",
    "    \n",
    "    # dangerous, territorial, dominant\n",
    "    data_train['Dangerous_dog'] = data_train['Breed'].map(lambda x:1 if ('Staffordshire Terrier' in x or 'Pit Bull' in x or 'Pitbull' in x or 'Dane' in x or 'Boxer' in x or 'Doberman' in x or 'Rottweiler' in x or 'Bull Terrier' in x or 'Bulldog'in x or 'Mastiff' in x or 'Dogue' in x) else 0)\n",
    "    \n",
    "    # hyper, energetic, destructive, working, hunting\n",
    "    data_train['Destructive'] = data_train['Breed'].map(lambda x:1 if ('Australian Cattle' in x or 'Australian Shephard' in x or 'Greyhound' in x or 'German Shepherd' in x or 'Dachshund' in x or 'Jack Russell' in x or 'Beagle' in x or 'Spaniel'in x or 'Cairn Terrier' in x or 'Pointer' in x or 'Plott Hound' in x or 'Great Pyrenees' in x or 'Collie' in x or 'Corgi' in x) else 0)\n",
    "\n",
    "    # dogs that love children\n",
    "    data_train['Loving_dog'] = data_train['Breed'].map(lambda x:1 if ('Retriever' in x or 'Labrador' in x or 'Miniature Poodle' in x or 'Poodle' in x) else 0)\n",
    "    \n",
    "    # small lap breeds\n",
    "    data_train['Small'] = data_train['Breed'].map(lambda x:1 if ('Chihuahua' in x or 'Pug' in x or 'Shih Tzu' in x or 'Schnauzer' in x or 'Maltese' in x or 'Lhasa Apso' in x) else 0)\n",
    "\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    # Convert 'Name' to 1/0\n",
    "\n",
    "    data_train['Name']= data_train['Name'].fillna(0)\n",
    "    data_train['Name'] = data_train['Name'].map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "    \n",
    "    data_train.fillna(data_train.mean(), inplace=True)\n",
    "    #dropped HOUR\n",
    "    X_df = data_train[['AnimalType','Name', 'Sex','S/N','Age','Year','Month', 'Day', 'Black/White', 'Tricolor', 'BW Tbb/BW Tbb White', 'White', 'Black', 'Org Tbb/Org Tbb White', 'Brown', 'Tortoise/Blk BW', 'Red/Red White', 'Tan', 'BW White/Seal Pt', 'Blue', 'Blue Tbb/Blue Tbb White', 'BW/White', 'White/Tan','Domes Sh/h','Domes M/h', 'Domes L/h', 'Siamese/Snowshow', 'Dangerous_dog', 'Destructive', 'Loving_dog', 'Small']]  \n",
    "    X = X_df.as_matrix()\n",
    "    \n",
    "    if has_outcome:\n",
    "        unique_label = sorted(list(set(np.asarray(data_train['OutcomeType']))))\n",
    "        data_train['OutcomeType'] = data_train['OutcomeType'].map(lambda x: unique_label.index(x))\n",
    "        y = np.asarray(data_train['OutcomeType'])\n",
    "        \n",
    "        return X, y, X_df\n",
    "    else:\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(y):\n",
    "    submission_file = open('/Users/Hooshmand/Desktop/Kaggle_Animal_shelter/sub.csv', 'w')\n",
    "    #labels = ['Adoption', 'Died', 'Euthanasia', 'Return_to_owner', 'Transfer']\n",
    "    submission_file.write('ID,Adoption,Died,Euthanasia,Return_to_owner,Transfer\\n')\n",
    "    for i, prob in enumerate(y):\n",
    "        entry = [i+1] + list(np.around(prob, decimals=3))\n",
    "        #entry[label + 1] = 1\n",
    "        submission_file.write(str(entry).replace('[', '').replace(']', '')+'\\n')\n",
    "    submission_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_df = animal_feature_transform(data_train)\n",
    "X_test = animal_feature_transform(data_test, has_outcome=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.2, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=254,\n",
       "              presort='auto', random_state=241, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=254, random_state=241, learning_rate=0.2)    \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67746642223801867"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train).score(X_train, y_train)\n",
    "# 0.67746642223801867 with hour\n",
    "# 0.67746642223801867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02251449,  0.04309179,  0.02055669,  0.05970022,  0.21806364,\n",
       "        0.06642588,  0.14581638,  0.105837  ,  0.01126751,  0.01203369,\n",
       "        0.00960881,  0.01429326,  0.01014265,  0.00574786,  0.01686928,\n",
       "        0.01378339,  0.00928846,  0.01393244,  0.01529781,  0.00576774,\n",
       "        0.0089562 ,  0.01092484,  0.00997178,  0.01818138,  0.00808242,\n",
       "        0.03012908,  0.00783924,  0.02771053,  0.01927857,  0.0190446 ,\n",
       "        0.01984237])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_\n",
    "#clf.feature_importances_>0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = clf.predict_proba(X_test)\n",
    "create_submission(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "#Train-Test split\n",
    "X_ttrain, X_ttest, y_ttrain, y_ttest = train_test_split(X_train, y_train, test_size = 0.8, random_state = 241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 0.999586424856\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting######################################################################################################################\n",
    "#test loss\n",
    "score = []\n",
    "clf = GradientBoostingClassifier(n_estimators=450, random_state=241, learning_rate=0.2)    \n",
    "clf.fit(X_ttrain, y_ttrain)\n",
    "for i, y_decision in enumerate(clf.staged_decision_function(X_ttest)):\n",
    "    y_pred = 1.0 / (1.0 + np.exp(-y_decision))\n",
    "    loss = log_loss(y_ttest, y_pred)\n",
    "    score.append(loss)\n",
    "print np.argmin(score), np.min(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.03"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=254, random_state=241)\n",
    "clf.fit(X_ttrain, y_ttrain)\n",
    "pred = clf.predict_proba(X_ttest)\n",
    "round(log_loss(y_ttest, pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-5755c4faa2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_decision\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstaged_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_decision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting######################################################################################################################\n",
    "#test loss\n",
    "score = []\n",
    "clf = GradientBoostingClassifier(n_estimators=450, random_state=241, learning_rate=0.2)    \n",
    "clf.fit(X_train, y_train)\n",
    "for i, y_decision in enumerate(clf.staged_decision_function(X_test)):\n",
    "    y_pred = 1.0 / (1.0 + np.exp(-y_decision))\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    score.append(loss)\n",
    "print np.argmin(score), np.min(score)\n",
    "#242 1.00030373454\n",
    "#254 0.999639824874"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
